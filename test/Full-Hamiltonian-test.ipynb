{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from line_profiler import LineProfiler\n",
    "from loguru import logger\n",
    "from torch import optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from pyscf import fci\n",
    "\n",
    "from utils import setup_seed, Logger, ElectronInfo, Dtype, state_to_string\n",
    "from utils.pyscf_helper import read_integral, interface\n",
    "from utils import convert_onv, get_fock_space\n",
    "from utils.det_helper import DetLUT, select_det, sort_det\n",
    "from utils.distributed import get_rank\n",
    "from utils.loggings import dist_print\n",
    "from utils.pyscf_helper.dice_pyscf import read_dice_wf, run_shci\n",
    "from vmc.ansatz import DecoderWaveFunction\n",
    "from vmc.optim import VMCOptimizer\n",
    "from ci_vmc.hybrid import NqsCi\n",
    "from ci import unpack_ucisd, ucisd_to_fci, fci_revise, CIWavefunction\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "atom: str = \"\"\n",
    "bond = 1.50\n",
    "for k in range(6):\n",
    "    atom += f\"H, 0.00, 0.00, {k * bond:.3f} ;\"\n",
    "integral_file = tempfile.mkstemp()[1]\n",
    "sorb, nele, e_lst, fci_amp, ucisd_amp, mf = interface(\n",
    "    atom, integral_file=integral_file, cisd_coeff=True,\n",
    "    basis=\"sto-3g\",\n",
    "    localized_orb=False,\n",
    "    localized_method=\"meta-lowdin\",\n",
    ")\n",
    "logger.info(e_lst)\n",
    "h1e, h2e, ci_space, ecore, sorb = read_integral(\n",
    "    integral_file,\n",
    "    nele,\n",
    "    # save_onstate=True,\n",
    "    # external_onstate=\"profiler/H12-1.50\",\n",
    "    # ##given_sorb= (nele + 2),\n",
    "    device=device,\n",
    "    # prefix=\"test-onstate\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.public_function import get_fock_space\n",
    "from utils.public_function import torch_sort_onv\n",
    "\n",
    "from libs.C_extension import get_comb_tensor, get_hij_torch, onv_to_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fock_space = get_fock_space(sorb=sorb)\n",
    "idx = torch_sort_onv(fock_space)\n",
    "fock_space = fock_space[idx]\n",
    "# fock_space.numpy().view(np.uint64)\n",
    "fock_space_state = ((onv_to_tensor(fock_space, sorb=sorb) + 1)/2).to(torch.int64)\n",
    "fock_space_state, fock_space.numpy().view(np.uint64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = fock_space.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ham = torch.zeros(dim, dim, dtype=torch.double)\n",
    "for i in range(0, fock_space.shape[0]):\n",
    "    x = fock_space_state[i]\n",
    "    alpha = x[::2].sum().item()\n",
    "    beta = x[1::2].sum().item()\n",
    "    nele = alpha + beta\n",
    "    comb = get_comb_tensor(fock_space[i].reshape(1, -1), sorb, nele, alpha, beta, True)[0]\n",
    "    # print(comb.shape)\n",
    "    # print(comb.squeeze(0))\n",
    "    hij = get_hij_torch(fock_space[i].reshape(1, -1), comb.squeeze(0),h1e, h2e, sorb, nele)\n",
    "    # print(hij)\n",
    "    Ham[i][comb.view(torch.int64).flatten()] = hij\n",
    "x = torch.linalg.eigh(Ham)\n",
    "e_fci = e_lst[0]\n",
    "assert abs(x[0][0].item() + ecore - e_fci) < 1.0e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ham_cuda = torch.zeros(dim, dim, dtype=torch.double, device=\"cuda\")\n",
    "fock_space_cuda = fock_space.to(\"cuda\")\n",
    "fock_space_state_cuda = fock_space_state.to(\"cuda\")\n",
    "h1e_cuda = h1e.to(\"cuda\")\n",
    "h2e_cuda = h2e.to(\"cuda\")\n",
    "\n",
    "for i in range(0, fock_space.shape[0]):\n",
    "    x = fock_space_state_cuda[i]\n",
    "    alpha = x[::2].sum().item()\n",
    "    beta = x[1::2].sum().item()\n",
    "    nele = alpha + beta\n",
    "    comb = get_comb_tensor(fock_space_cuda[i].reshape(1, -1), sorb, nele, alpha, beta, True)[0]\n",
    "    # print(comb.shape)\n",
    "    # print(comb.squeeze(0))\n",
    "    hij = get_hij_torch(fock_space_cuda[i].reshape(1, -1), comb.squeeze(0), h1e_cuda, h2e_cuda, sorb, nele)\n",
    "    # print(hij)\n",
    "    Ham_cuda[i][comb.view(torch.int64).flatten()] = hij\n",
    "x = torch.linalg.eigh(Ham_cuda)\n",
    "e_fci = e_lst[0]\n",
    "assert abs(x[0][0].item() + ecore - e_fci) < 1.0e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "col = []\n",
    "row = []\n",
    "\n",
    "for i in range(0, fock_space.shape[0]):\n",
    "    x = fock_space_state[i]\n",
    "    alpha = x[::2].sum().item()\n",
    "    beta = x[1::2].sum().item()\n",
    "    nele = alpha + beta\n",
    "    comb = get_comb_tensor(fock_space[i].reshape(1, -1), sorb, nele, alpha, beta, True)[0]\n",
    "    # print(comb.shape)\n",
    "    # print(comb.squeeze(0))\n",
    "    hij = get_hij_torch(fock_space[i].reshape(1, -1), comb.squeeze(0),h1e, h2e, sorb, nele)\n",
    "    # print(hij)\n",
    "    data.append(hij.cpu().numpy().flatten())\n",
    "    col.append(np.array([i] * comb.size(1), dtype=np.int64))\n",
    "    row.append(comb.cpu().numpy().view(np.uint64).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[3], col[3], row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate(data)\n",
    "col = np.concatenate(col)\n",
    "row = np.concatenate(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "x = csr_matrix((data, (row, col)), shape=(dim, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.sparse.linalg.eigsh(x)[0][0] + ecore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hij = get_hij_torch(ci_space, ci_space, h1e, h2e, 12, 6)\n",
    "\n",
    "torch.linalg.eigh(hij)[0][0].item() + ecore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
