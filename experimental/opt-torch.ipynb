{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time  \n",
    "import numpy as np \n",
    "from torch import nn, Tensor, optim\n",
    "from torch.nn.parameter import Parameter \n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "from typing import List, Optional, Callable, Tuple\n",
    "\n",
    "import libs.hij_tensor as pt\n",
    "import libs.py_fock as fock\n",
    "import libs.py_integral as integral\n",
    "\n",
    "from vmc.PublicFunction import check_para, unit8_to_bit, setup_seed\n",
    "from vmc.ansatz import rRBMWavefunction\n",
    "from vmc.optim import SR\n",
    "from vmc.eloc import local_energy, total_energy\n",
    "from vmc.sample import MCMCSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"pwd\") \n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]= '1'\n",
    "torch.set_default_dtype(torch.double)\n",
    "torch.set_printoptions(precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_lst(sorb: int, string: str):\n",
    "    arr = np.array(list(map(int, string)))[::-1]\n",
    "    lst = [0] * ((sorb-1)//64 +1)*8\n",
    "    for i in range((sorb-1)//8+1):\n",
    "        begin = i * 8\n",
    "        end = (i+1) * 8 if (i+1)*8 < sorb else sorb\n",
    "        idx = arr[begin:end]\n",
    "        lst[i] = np.sum(2**np.arange(len(idx)) * idx)\n",
    "\n",
    "    return lst\n",
    "\n",
    "chain_len = 2\n",
    "integral_file = f\"../integral/rmole-H2-0.734.info\"\n",
    "int2e, int1e, ecore = integral.load(integral.two_body(), integral.one_body(), 0.0, integral_file)\n",
    "print(ecore)\n",
    "sorb = int2e.sorb\n",
    "nele = 2\n",
    "alpha_ele = nele//2 \n",
    "beta_ele = nele//2\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "space = fock.get_fci_space(int(sorb//2), alpha_ele, beta_ele)\n",
    "dim = len(space)\n",
    "\n",
    "# h1e/h2e \n",
    "h1e = torch.tensor(int1e.data, dtype=torch.float64).to(device)\n",
    "h2e = torch.tensor(int2e.data, dtype=torch.float64).to(device)\n",
    "\n",
    "# bra/ket\n",
    "lst = []\n",
    "for i in range(dim):\n",
    "    lst.append(string_to_lst(sorb, space[i].to_string()))\n",
    "onstate1 = torch.tensor(lst, dtype=torch.uint8).to(device)\n",
    "# onstate2 = torch.tensor(lst, dtype=torch.uint8).to(device)\n",
    "print(onstate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivs(derivs, Eloc, N_s, p):\n",
    "    \"\"\"Computes variational derivatives and update to params\"\"\"\n",
    "    \n",
    "    # print(sigmas)\n",
    "    # theta = np.dot(weights.transpose(),\n",
    "    #                sigmas.transpose()) + B  # n_h x N_s\n",
    "    # dA = sigmas.transpose()  # n_spins x N_s\n",
    "    # dB = np.tanh(theta)  # n_h x N_s\n",
    "    # dW = sigmas.transpose().reshape((n_spins, 1, N_s)) * \\\n",
    "    #      np.tanh(theta.reshape((1, n_h, N_s)))\n",
    "    # derivs = np.concatenate([dA, dB, dW.reshape(n_spins * n_h, N_s)])\n",
    "   \n",
    "    print(f\"derivs shape {derivs.shape}\")\n",
    "    # print(f\"N_s {N_s}\")\n",
    "\n",
    "    avg_derivs = np.sum(derivs, axis=1, keepdims=True) / N_s\n",
    "    avg_derivs_mat = np.conjugate(avg_derivs.reshape(derivs.shape[0], 1))\n",
    "    # print(np.allclose(\n",
    "    #     avg_derivs_mat.real,\n",
    "    #     avg_derivs.real\n",
    "    # ))\n",
    "\n",
    "    # print(avg_derivs_mat.shape)\n",
    "    avg_derivs_mat = avg_derivs_mat * avg_derivs.reshape(\n",
    "        1, derivs.shape[0]\n",
    "    )\n",
    "    # print(derivs.shape, avg_derivs.shape, avg_derivs_mat.shape)\n",
    "    moment2 = np.einsum('ik,jk->ij', np.conjugate(derivs), derivs) / N_s\n",
    "    # print(f\"moment2: {moment2}\")\n",
    "    S_kk = np.subtract(moment2, avg_derivs_mat)\n",
    "    # print(f\"moment2 shape {moment2.shape}\")\n",
    "    # print(Eloc.shape)\n",
    "    F_p = np.sum(Eloc.transpose() * np.conjugate(derivs), axis=1) / N_s\n",
    "    F_p -= np.sum(Eloc.transpose(), axis=1) * \\\n",
    "           np.sum(np.conjugate(derivs), axis=1) / (N_s ** 2)\n",
    "    print(f\"F_p shape {F_p.shape}\")\n",
    "    S_kk2 = np.zeros(S_kk.shape, dtype=complex)\n",
    "    row, col = np.diag_indices(S_kk.shape[0])\n",
    "    S_kk2[row, col] = 0.02 # np.diagonal(S_kk) * regular(p)\n",
    "    S_reg = S_kk + S_kk2\n",
    "    update = np.dot(np.linalg.inv(S_reg), F_p).reshape(derivs.shape[0], 1)\n",
    "    \n",
    "    # print(f\"S_reg shape {S_reg.shape}\")\n",
    "    # print(f\"S_kk shape {S_kk.shape}\")\n",
    "    # print(f\"F_p shape {F_p.shape}\")\n",
    "    # print(f\"S_reg-1: {np.linalg.inv(S_reg).shape}\")\n",
    "    # print(f\"update shape {update.shape}\")\n",
    "    # print(f\"ssss {np.dot(np.linalg.inv(S_reg), F_p).shape}\") \n",
    "    return update\n",
    "\n",
    "def regular(p, l0=100, b=0.9, l_min=1e-4):\n",
    "    \"\"\"\n",
    "    Lambda regularization parameter for S_kk matrix,\n",
    "    see supplementary materials\n",
    "    \"\"\"\n",
    "\n",
    "    return max(l0 * (b**p) , l_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sr(eloc: Tensor, grad_total: Tensor, N_state: int, p: int, \n",
    "      debug=False, L2_penalty: Tensor = None, opt_gd = False) -> Tensor:\n",
    "\n",
    "\n",
    "    avg_grad = torch.sum(grad_total, axis=0, keepdim=True)/N_state\n",
    "    avg_grad_mat = avg_grad.reshape(-1, 1)\n",
    "    # avg_grad_mat = torch.conj(avg_grad.reshape(-1, 1))\n",
    "    avg_grad_mat = avg_grad_mat * avg_grad.reshape(1, -1)\n",
    "    # moment2 = torch.einsum(\"ki, kj->ij\", torch.conj(grad_total), grad_total)/N_state\n",
    "    moment2 = torch.einsum(\"ki, kj->ij\", grad_total, grad_total)/N_state\n",
    "    S_kk = torch.subtract(moment2, avg_grad_mat)\n",
    "    \n",
    "    F_p = torch.sum(eloc.transpose(1, 0) * grad_total, axis=0)/N_state\n",
    "    F_p -= torch.sum(eloc.transpose(1, 0), axis=0) * torch.sum(grad_total, axis=0)/(N_state**2)\n",
    "    if L2_penalty is not None:\n",
    "        # print(f\"L2 re: \\n {L2_penalty}\")\n",
    "        F_p += L2_penalty\n",
    "    # F_p = torch.sum(eloc.transpose(1, 0) * torch.conj(grad_total), axis=0)/N_state\n",
    "    # F_p -= torch.sum(eloc.transpose(1, 0), axis=0) * torch.sum(torch.conj(grad_total), axis=0)/(N_state**2)\n",
    "\n",
    "    if opt_gd:\n",
    "        update = F_p\n",
    "    else:\n",
    "        S_kk2 = torch.eye(S_kk.shape[0], dtype=S_kk.dtype, device=S_kk.device) * 0.02\n",
    "        # S_kk2 = regular(p) * torch.diag(S_kk)\n",
    "        S_reg = S_kk + S_kk2\n",
    "        # if debug:\n",
    "        #     print(f\"S_kk.-1\", torch.linalg.inv(S_reg))\n",
    "        update = torch.matmul(torch.linalg.inv(S_reg), F_p).reshape(1, -1)\n",
    "        # update = torch.matmul(torch.linalg.inv(torch.eye(S_kk.shape[0], dtype=torch.double)), F_p).reshape(1, -1)\n",
    "    \n",
    "    return update\n",
    "\n",
    "def calculate_sr_grad(params: List[Tensor], \n",
    "                      grad_save: List[Tensor],\n",
    "                      eloc: Tensor, \n",
    "                      N_state: int,\n",
    "                      p: int,\n",
    "                      opt_gd = False, \n",
    "                      lr: float = 0.02):\n",
    "    n_para = len(grad_save)\n",
    "    param_group = list(params)\n",
    "    for i in range(n_para):\n",
    "\n",
    "        L2 = 0.001 * (param_group[i].detach().clone()**2).reshape(-1)\n",
    "        shape = param_group[i].shape\n",
    "        dlnpsi = grad_save[i].reshape(N_state, -1) # (N_state, N_para) two dim \n",
    "        # print(f\"dlnpis shape {dlnpsi.shape}\")\n",
    "        # print(dlnpsi)\n",
    "        update = sr(eloc, dlnpsi, N_state, p, debug = (i==1), L2_penalty=L2, opt_gd=opt_gd)\n",
    "        # print(f\"grad_comb {grad_comb_lst[i]}\")\n",
    "        # update1 = compute_derivs(grad_comb_lst[i].T.cpu().detach().numpy(), eloc.T.cpu().detach().numpy(), N_state, p)\n",
    "        # print(\"sssss\", np.allclose(\n",
    "        #     update.detach().cpu().numpy(),\n",
    "        #     update1.real.T\n",
    "        # ))\n",
    "        if p >= 100:\n",
    "            if i >= 0:\n",
    "                print(f\"{i}th para\")\n",
    "                print(f\"parameter in model\\n {param_group[i]}\")\n",
    "                print(f\"dpsi \\n {param_group[i].grad}\")\n",
    "                print(f\"dlnpsi \\n{dlnpsi}\")\n",
    "                print(f\"update * -lr \\n{update.reshape(shape)*(-lr)}\")\n",
    "        # param_group[i].data = param_group[i].data.add(update.reshape(shape_lst[i]), alpha=-lr)\n",
    "        param_group[i].data.add_(update.reshape(shape), alpha=-lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecore = 0.00\n",
    "seed = 42\n",
    "setup_seed(seed)\n",
    "e_list =[]\n",
    "model = rRBMWavefunction(sorb, sorb*2, init_weight=0.001).to(device)\n",
    "print(model)\n",
    "analytic_derivative = True\n",
    "time_sample = []\n",
    "time_iter = []\n",
    "# print(model(unit8_to_bit(onstate1, sorb)))\n",
    "\n",
    "n = 60\n",
    "debug = True\n",
    "N = onstate1.shape[0] if debug else n\n",
    "print(onstate1.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    nbatch =  len(onstate1)\n",
    "    e = total_energy(onstate1, nbatch, h1e, h2e, \n",
    "                                 model,ecore, sorb, nele, exact=debug)\n",
    "    e_list.append(e.item())\n",
    "print(f\"begin e is {e}\")\n",
    "\n",
    "from vmc.optim import SR\n",
    "\n",
    "opt = SR(model.parameters(), lr=0.005, N_state=N)\n",
    "\n",
    "for p in range(5000):\n",
    "    if p <= 800:\n",
    "        initial_state = onstate1[random.randrange(len(onstate1))].clone().detach()\n",
    "    else:\n",
    "        initial_state = onstate1[0].clone().detach()\n",
    "    \n",
    "    dln_grad_lst = []\n",
    "    out_lst = []\n",
    "    t0 = time.time_ns()\n",
    "    sample = MCMCSampler(model, initial_state, h1e, h2e , n, sorb, nele, \n",
    "                        verbose=True, debug_exact=debug, full_space=onstate1)\n",
    "    state, eloc = sample.run() # eloc [n_sample]\n",
    "    n_sample = len(state)\n",
    "    # print(\"state: \")\n",
    "    # print(state)\n",
    "    # print(\"local energy\")\n",
    "    # print(eloc)\n",
    "    # TODO: cuda version unit8_to_bit 2D\n",
    "    sample_state = unit8_to_bit(state, sorb)\n",
    "    delta = (time.time_ns() - t0)/1.00E09\n",
    "    time_sample.append(delta)\n",
    "\n",
    "    if analytic_derivative:\n",
    "        model.zero_grad()\n",
    "        grad_sample = model(sample_state, dlnPsi=True) \n",
    "        # tuple, length: n_para, shape: (n_sample, param.shape),\n",
    "    else:\n",
    "        for i in range(n_sample):\n",
    "            model.zero_grad()\n",
    "            # handle = model.register_full_backward_hook(hook_fn_backward)\n",
    "            psi = model(sample_state[i].requires_grad_())\n",
    "            out_lst.append(psi.detach().clone())\n",
    "            psi.backward()\n",
    "            lst = []\n",
    "            for para in model.parameters():\n",
    "                if para.grad is not None:\n",
    "                    lst.append(para.grad.detach().clone()/psi.detach().clone())\n",
    "            dln_grad_lst.append(lst)\n",
    "            # handle.remove()\n",
    "            del psi, lst\n",
    "\n",
    "        # print(\"psi:\")\n",
    "        # print(torch.tensor(out_lst))\n",
    "        # combine all sample grad => tuple, length: n_para (N_sample, n_para)\n",
    "        n_para = len(list(model.parameters()))\n",
    "        grad_comb_lst = []\n",
    "        for i in range(n_para):\n",
    "            comb = []\n",
    "            for j in range(n_sample):\n",
    "                comb.append(dln_grad_lst[j][i].reshape(1, -1)) \n",
    "            grad_comb_lst.append(torch.cat(comb)) # (n_sample, n_para)\n",
    "\n",
    "        grad_sample = grad_comb_lst\n",
    "    if p >= 530:\n",
    "        print(\"22222\")\n",
    "    # print(f\"dln_psi\")\n",
    "    # for i in range(n_para):\n",
    "    #     print(grad_sample[i][:10])\n",
    "    calculate_sr_grad(model.parameters(), grad_sample, eloc.reshape(1, -1), n_sample, p+1, lr=0.010, opt_gd=True)\n",
    "    opt.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nbatch = n_sample\n",
    "        e = total_energy(state.detach(), nbatch, h1e, h2e, \n",
    "                                 model, ecore, sorb, nele, exact=debug)\n",
    "        e_list.append(e.item())\n",
    "    print(f\"{p} iteration total energy is {e:.5f} \\n\")\n",
    "    time_iter.append((time.time_ns() - t0)/1.00E09)\n",
    "    del dln_grad_lst, out_lst\n",
    "\n",
    "\n",
    "# x0 = torch.tensor([1.0, 1.0, -1.0, -1.0], dtype=torch.float, requires_grad=True).to(device)\n",
    "# handle = net.register_full_backward_hook(hook_fn_backward)\n",
    "# y = net(x0)\n",
    "# y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "e = np.array(e_list)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(np.arange(len(e)), e)\n",
    "print(e[-10])\n",
    "plt.show()\n",
    "plt.savefig(r\"H2-0.735.png\", dpi=1000)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c763cec028a98f3e988162684808c3005c60a7dccfebd0b415fd12b14c9f0b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
